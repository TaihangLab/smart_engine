"""
ç°ä»£åŒ–LLMæœåŠ¡ - åŸºäºLangChain 1.0.7 + OpenAIå…¼å®¹API
===========================================================
è®¾è®¡ç†å¿µï¼š
1. é…ç½®é©±åŠ¨ï¼šæ‰€æœ‰æ¨¡å‹é…ç½®åœ¨config.pyä¸­ç»Ÿä¸€ç®¡ç†
2. æ™ºèƒ½è·¯ç”±ï¼šæ ¹æ®è¾“å…¥ç±»å‹ï¼ˆçº¯æ–‡æœ¬/å›¾ç‰‡/è§†é¢‘ï¼‰è‡ªåŠ¨é€‰æ‹©æ¨¡å‹
3. è§†é¢‘æ”¯æŒï¼šä½¿ç”¨OpenAI frame_listæ ¼å¼å¤„ç†è§†é¢‘åºåˆ—
4. ç°ä»£APIï¼šåŸºäºLangChain 1.0.7æœ€æ–°ç‰¹æ€§

è·¯ç”±è§„åˆ™ï¼š
- çº¯æ–‡æœ¬è¾“å…¥ â†’ TEXT_LLM_* (åƒé—®3)
- å›¾ç‰‡/è§†é¢‘è¾“å…¥ â†’ MULTIMODAL_LLM_* (åƒé—®3VL)
- å¤±è´¥æ—¶è‡ªåŠ¨é™çº§åˆ°BACKUP_*
"""
import logging
import base64
import json
import re
from typing import Dict, Any, Optional, List, Union
from io import BytesIO
from PIL import Image
import numpy as np

# LangChain 1.0.7+
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.language_models import BaseChatModel
from langchain_openai import ChatOpenAI

# æ³¨æ„ï¼švllmçš„OpenAIå…¼å®¹APIç›´æ¥æ”¯æŒè§†é¢‘å¸§åˆ—è¡¨ï¼Œæ— éœ€qwen_vl_utils
# qwen_vl_utilsä»…ç”¨äºæœ¬åœ°transformersæ¨¡å‹åŠ è½½

# é¡¹ç›®æ¨¡å—
from app.core.config import settings
from app.services.redis_client import redis_client

logger = logging.getLogger(__name__)


class LLMServiceResult:
    """LLMæœåŠ¡è°ƒç”¨ç»“æœ"""
    
    def __init__(self, success: bool, response: Optional[str] = None, 
                 analysis_result: Optional[Dict] = None,
                 error_message: Optional[str] = None):
        self.success = success
        self.response = response
        self.analysis_result = analysis_result or {}
        self.error_message = error_message
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "success": self.success,
            "response": self.response,
            "analysis_result": self.analysis_result,
            "error_message": self.error_message
        }


class RedisMemoryStore:
    """åŸºäºRedisçš„æ¶ˆæ¯å†å²å­˜å‚¨"""
    
    def __init__(self, redis_client, ttl: int = 7 * 24 * 3600):
        self.redis_client = redis_client
        self.ttl = ttl
        self.prefix = "llm_chat_history:"
    
    def get_messages(self, session_id: str) -> List[BaseMessage]:
        """è·å–ä¼šè¯æ¶ˆæ¯å†å²"""
        try:
            key = f"{self.prefix}{session_id}"
            messages_data = self.redis_client.lrange(key, 0, -1)
            
            messages = []
            for message_json in messages_data:
                try:
                    message_dict = json.loads(message_json)
                    message_type = message_dict.get('type', 'human')
                    content = message_dict.get('content', '')
                    
                    if message_type == 'human':
                        messages.append(HumanMessage(content=content))
                    elif message_type == 'ai':
                        messages.append(AIMessage(content=content))
                    elif message_type == 'system':
                        messages.append(SystemMessage(content=content))
                except (json.JSONDecodeError, KeyError) as e:
                    logger.warning(f"è§£ææ¶ˆæ¯å¤±è´¥: {e}")
                    continue
            
            return messages
        except Exception as e:
            logger.error(f"è·å–ä¼šè¯å†å²å¤±è´¥: {e}")
            return []
    
    def add_message(self, session_id: str, message: BaseMessage) -> None:
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²"""
        try:
            key = f"{self.prefix}{session_id}"
            message_type = 'human' if isinstance(message, HumanMessage) else \
                          'ai' if isinstance(message, AIMessage) else 'system'
            
            message_data = json.dumps({
                'type': message_type,
                'content': message.content
            })
            
            self.redis_client.rpush(key, message_data)
            self.redis_client.expire(key, self.ttl)
        except Exception as e:
            logger.error(f"æ·»åŠ æ¶ˆæ¯åˆ°å†å²å¤±è´¥: {e}")
    
    def clear(self, session_id: str) -> None:
        """æ¸…é™¤ä¼šè¯å†å²"""
        try:
            key = f"{self.prefix}{session_id}"
            self.redis_client.delete(key)
        except Exception as e:
            logger.error(f"æ¸…é™¤ä¼šè¯å†å²å¤±è´¥: {e}")


class RedisChatMessageHistory(BaseChatMessageHistory):
    """RedisèŠå¤©æ¶ˆæ¯å†å²å®ç°ï¼ˆLangChain 1.0.7å…¼å®¹ï¼‰"""
    
    def __init__(self, session_id: str, memory_store: RedisMemoryStore):
        self.session_id = session_id
        self.memory_store = memory_store
        self._messages = None
    
    @property
    def messages(self) -> List[BaseMessage]:
        """è·å–æ¶ˆæ¯åˆ—è¡¨"""
        self._messages = self.memory_store.get_messages(self.session_id)
        return self._messages
    
    def add_message(self, message: BaseMessage) -> None:
        """æ·»åŠ æ¶ˆæ¯"""
        self.memory_store.add_message(self.session_id, message)
        self._messages = None
    
    def clear(self) -> None:
        """æ¸…é™¤å†å²"""
        self.memory_store.clear(self.session_id)
        self._messages = []


class LLMService:
    """
    é…ç½®é©±åŠ¨çš„æ™ºèƒ½LLMæœåŠ¡ - LangChain 1.0.7 + OpenAIå…¼å®¹API
    ===========================================================
    ç‰¹ç‚¹ï¼š
    1. é›¶ç¡¬ç¼–ç ï¼šæ‰€æœ‰æ¨¡å‹é…ç½®æ¥è‡ªconfig.py
    2. æ™ºèƒ½è·¯ç”±ï¼šè‡ªåŠ¨æ ¹æ®è¾“å…¥ç±»å‹é€‰æ‹©æœ€ä½³æ¨¡å‹
    3. è§†é¢‘æ”¯æŒï¼šä½¿ç”¨OpenAI frame_listæ ¼å¼å¤„ç†è§†é¢‘å¸§
    4. è‡ªåŠ¨é™çº§ï¼šä¸»æ¨¡å‹æ•…éšœæ—¶è‡ªåŠ¨åˆ‡æ¢å¤‡ç”¨
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.memory_store = RedisMemoryStore(redis_client)
        self._client_cache = {}
        
        self.logger.info("ğŸš€ LLMæœåŠ¡åˆå§‹åŒ– (LangChain 1.0.7 + OpenAIå…¼å®¹API)")
        self.logger.info(f"   çº¯æ–‡æœ¬æ¨¡å‹: {settings.TEXT_LLM_MODEL} @ {settings.TEXT_LLM_BASE_URL}")
        self.logger.info(f"   å¤šæ¨¡æ€æ¨¡å‹: {settings.MULTIMODAL_LLM_MODEL} @ {settings.MULTIMODAL_LLM_BASE_URL}")
        self.logger.info(f"   è§†é¢‘æ”¯æŒ: OpenAI frame_listæ ¼å¼")
    
    def _get_client(self, model_type: str = "text", use_backup: bool = False) -> ChatOpenAI:
        """
        è·å–LLMå®¢æˆ·ç«¯ï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            model_type: "text" æˆ– "multimodal"
            use_backup: æ˜¯å¦ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
        """
        cache_key = f"{model_type}_{use_backup}"
        
        if cache_key in self._client_cache:
            return self._client_cache[cache_key]
        
        # æ ¹æ®æ¨¡å‹ç±»å‹å’Œæ˜¯å¦å¤‡ç”¨é€‰æ‹©é…ç½®
        if model_type == "multimodal":
            if use_backup:
                base_url = settings.BACKUP_MULTIMODAL_LLM_BASE_URL
                model = settings.BACKUP_MULTIMODAL_LLM_MODEL
                api_key = "ollama"
            else:
                base_url = settings.MULTIMODAL_LLM_BASE_URL
                model = settings.MULTIMODAL_LLM_MODEL
                api_key = settings.MULTIMODAL_LLM_API_KEY
        else:  # text
            if use_backup:
                base_url = settings.BACKUP_TEXT_LLM_BASE_URL
                model = settings.BACKUP_TEXT_LLM_MODEL
                api_key = "ollama"
            else:
                base_url = settings.TEXT_LLM_BASE_URL
                model = settings.TEXT_LLM_MODEL
                api_key = settings.TEXT_LLM_API_KEY
        
        # ä½¿ç”¨LangChain 1.0.7çš„ChatOpenAI
        client = ChatOpenAI(
            model=model,
            api_key=api_key,
            base_url=base_url,
            temperature=settings.LLM_TEMPERATURE,
            max_tokens=settings.LLM_MAX_TOKENS,
            timeout=settings.LLM_TIMEOUT
        )
        
        self._client_cache[cache_key] = client
        return client
    
    def _encode_image(self, image_data: Union[str, bytes, np.ndarray, Image.Image]) -> str:
        """
        å°†å›¾ç‰‡ç¼–ç ä¸ºbase64æˆ–URLå­—ç¬¦ä¸²
        
        æ”¯æŒæ ¼å¼ï¼š
        - str: æ–‡ä»¶è·¯å¾„ã€URLã€å·²ç¼–ç base64
        - bytes: åŸå§‹å›¾ç‰‡å­—èŠ‚
        - np.ndarray: OpenCVå›¾ç‰‡æ•°ç»„
        - PIL.Image: PILå›¾ç‰‡å¯¹è±¡
        """
        try:
            # å·²ç»æ˜¯URLæˆ–base64å­—ç¬¦ä¸²
            if isinstance(image_data, str):
                if image_data.startswith("http"):
                    return image_data  # URLç›´æ¥è¿”å›
                elif image_data.startswith("data:image"):
                    return image_data  # å·²ç¼–ç çš„data URL
                else:
                    # å°è¯•ä½œä¸ºæ–‡ä»¶è·¯å¾„è¯»å–
                    with open(image_data, "rb") as f:
                        image_data = f.read()
            
            # numpyæ•°ç»„è½¬PIL
            if isinstance(image_data, np.ndarray):
                if image_data.dtype != np.uint8:
                    image_data = (image_data * 255).astype(np.uint8) if image_data.max() <= 1.0 else image_data.astype(np.uint8)
                if len(image_data.shape) == 3 and image_data.shape[2] == 3:
                    # BGR to RGB (OpenCV uses BGR)
                    image_data = Image.fromarray(image_data[..., ::-1])
                else:
                    image_data = Image.fromarray(image_data)
            
            # PILå›¾ç‰‡è½¬bytes
            if isinstance(image_data, Image.Image):
                buffer = BytesIO()
                image_data.save(buffer, format="JPEG")
                image_data = buffer.getvalue()
            
            # bytesç¼–ç ä¸ºbase64
            if isinstance(image_data, bytes):
                encoded = base64.b64encode(image_data).decode('utf-8')
                return f"data:image/jpeg;base64,{encoded}"
            
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾ç‰‡æ ¼å¼: {type(image_data)}")
            
        except Exception as e:
            self.logger.error(f"å›¾ç‰‡ç¼–ç å¤±è´¥: {e}")
            raise
    
    def _build_messages_for_video(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        video_frames: Optional[List] = None,
        fps: Optional[float] = None
    ) -> List[BaseMessage]:
        """
        æ„å»ºè§†é¢‘åˆ†ææ¶ˆæ¯ï¼ˆOpenAIå…¼å®¹APIæ ¼å¼ï¼‰
        
        ä½¿ç”¨ {"type": "video", "video": [...], "fps": 2.0} æ ¼å¼
        è¿™æ˜¯ vllm çš„ OpenAI å…¼å®¹ API æ”¯æŒçš„æ ‡å‡†æ ¼å¼
        
        å‚è€ƒ: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html
        
        Args:
            prompt: æ–‡æœ¬æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            video_frames: è§†é¢‘å¸§åˆ—è¡¨ï¼ˆnumpy/PIL/URLå­—ç¬¦ä¸²ï¼‰
            fps: å¸§ç‡ï¼ˆå‘Šè¯‰æ¨¡å‹å¸§çš„æ—¶åºå¯†åº¦ï¼‰
        """
        messages = []
        
        # ç³»ç»Ÿæ¶ˆæ¯
        if system_prompt:
            messages.append(SystemMessage(content=system_prompt))
        
        # ç”¨æˆ·æ¶ˆæ¯ - OpenAIè§†é¢‘APIæ ¼å¼
        content = []
        
        # å°†æ‰€æœ‰å¸§ç¼–ç ä¸ºbase64
        if video_frames is not None and len(video_frames) > 0:
            frame_list = []
            for frame in video_frames:
                if isinstance(frame, str) and frame.startswith("http"):
                    frame_list.append(frame)  # URLç›´æ¥ä½¿ç”¨
                else:
                    frame_list.append(self._encode_image(frame))  # ç¼–ç ä¸ºbase64
            
            # OpenAIè§†é¢‘æ ¼å¼
            content.append({
                "type": "video",
                "video": frame_list,  # å¸§åˆ—è¡¨ï¼ˆbase64æˆ–URLï¼‰
                "fps": fps if fps else 2.0  # å¸§ç‡
            })
        
        # æ–‡æœ¬æç¤º
        content.append({
            "type": "text",
            "text": prompt
        })
        
        messages.append(HumanMessage(content=content))
        return messages
    
    def _build_messages_standard(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        image_data: Optional[Union[str, bytes, np.ndarray, Image.Image, List]] = None,
        video_frames: Optional[List] = None,
        fps: Optional[float] = None
    ) -> List[BaseMessage]:
        """
        æ ‡å‡†æ¶ˆæ¯æ„å»ºï¼ˆå…¼å®¹æ‰€æœ‰LangChainæ”¯æŒçš„æ ¼å¼ï¼‰
        """
        messages = []
        
        # ç³»ç»Ÿæ¶ˆæ¯
        if system_prompt:
            messages.append(SystemMessage(content=system_prompt))
        
        # ç”¨æˆ·æ¶ˆæ¯
        if video_frames is not None and len(video_frames) > 0:
            # è§†é¢‘å¸§åºåˆ—
            content = [{"type": "text", "text": prompt}]
            
            for idx, frame in enumerate(video_frames):
                encoded_frame = self._encode_image(frame)
                
                # æ·»åŠ æ—¶é—´æˆ³ï¼ˆå¦‚æœæä¾›fpsï¼‰
                if fps:
                    timestamp = idx / fps
                    content.append({
                        "type": "text",
                        "text": f"[å¸§ {idx+1}, æ—¶é—´ {timestamp:.2f}s]"
                    })
                
                content.append({
                    "type": "image_url",
                    "image_url": {"url": encoded_frame}
                })
            
            messages.append(HumanMessage(content=content))
            
        elif image_data is not None:
            # å•å›¾ç‰‡æˆ–å›¾ç‰‡åˆ—è¡¨
            if isinstance(image_data, list):
                content = [{"type": "text", "text": prompt}]
                for img in image_data:
                    encoded = self._encode_image(img)
                    content.append({
                        "type": "image_url",
                        "image_url": {"url": encoded}
                    })
                messages.append(HumanMessage(content=content))
            else:
                # å•å›¾ç‰‡
                encoded = self._encode_image(image_data)
                messages.append(HumanMessage(content=[
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": encoded}}
                ]))
        else:
            # çº¯æ–‡æœ¬
            messages.append(HumanMessage(content=prompt))
        
        return messages
    
    def _parse_json_response(self, response_text: str) -> Dict[str, Any]:
        """
        æ™ºèƒ½è§£æJSONå“åº”ï¼ˆå¤„ç†markdownåŒ…è£¹çš„JSONï¼‰
        """
        try:
            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            text = response_text.strip()
            if text.startswith("```"):
                text = re.sub(r'^```(?:json)?\s*\n', '', text)
                text = re.sub(r'\n```\s*$', '', text)
            
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(text)
            
        except json.JSONDecodeError as e:
            self.logger.warning(f"JSONè§£æå¤±è´¥: {e}")
            # å°è¯•æå–JSONå†…å®¹
            json_match = re.search(r'\{.*\}', text, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(0))
                except:
                    pass
            
            # è§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
            return {"raw_response": text, "parse_error": str(e)}
    
    def call_llm(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        image_data: Optional[Union[str, bytes, np.ndarray, Image.Image, List]] = None,
        video_frames: Optional[List] = None,
        fps: Optional[float] = None,
        response_format: Optional[Dict] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        use_video_format: bool = True,
        **kwargs
    ) -> LLMServiceResult:
        """
        ç»Ÿä¸€LLMè°ƒç”¨æ¥å£ - é…ç½®é©±åŠ¨æ™ºèƒ½è·¯ç”±
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            image_data: å•å¼ å›¾ç‰‡æˆ–å›¾ç‰‡åˆ—è¡¨
            video_frames: è§†é¢‘å¸§åºåˆ—ï¼ˆnumpy/PIL/URLï¼‰
            fps: è§†é¢‘å¸§ç‡ï¼ˆå‘Šè¯‰æ¨¡å‹æ—¶åºå¯†åº¦ï¼‰
            response_format: å“åº”æ ¼å¼ {"type": "json_object"}
            temperature: æ¸©åº¦å‚æ•°ï¼ˆè¦†ç›–é…ç½®ï¼‰
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆè¦†ç›–é…ç½®ï¼‰
            use_video_format: è§†é¢‘å¸§ä½¿ç”¨OpenAI videoæ ¼å¼ï¼ˆæ¨èTrueï¼‰
            
        Returns:
            LLMServiceResult: åŒ…å«å“åº”æ–‡æœ¬å’Œè§£æç»“æœ
        """
        try:
            # æ™ºèƒ½è·¯ç”±ï¼šåˆ¤æ–­ä½¿ç”¨å“ªç§æ¨¡å‹
            # å®‰å…¨æ£€æŸ¥æ˜¯å¦æœ‰è§†è§‰è¾“å…¥ï¼ˆé¿å…numpyæ•°ç»„çš„å¸ƒå°”è¿ç®—æ­§ä¹‰ï¼‰
            has_image = image_data is not None
            has_video = video_frames is not None and (isinstance(video_frames, list) and len(video_frames) > 0)
            has_visual_input = has_image or has_video
            
            model_type = "multimodal" if (settings.LLM_AUTO_ROUTING and has_visual_input) else "text"
            
            self.logger.debug(f"ğŸ¯ è·¯ç”±å†³ç­–: è¾“å…¥ç±»å‹={'å¤šæ¨¡æ€' if has_visual_input else 'çº¯æ–‡æœ¬'}, æ¨¡å‹={model_type}")
            
            # æ„å»ºæ¶ˆæ¯
            if has_video and use_video_format:
                # ä½¿ç”¨OpenAIè§†é¢‘æ ¼å¼ï¼ˆæ¨èï¼‰
                messages = self._build_messages_for_video(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    video_frames=video_frames,
                    fps=fps
                )
                self.logger.debug(f"ğŸ“¹ ä½¿ç”¨OpenAIè§†é¢‘æ ¼å¼å¤„ç†{len(video_frames)}å¸§")
            else:
                # æ ‡å‡†æ¶ˆæ¯æ„å»ºï¼ˆå•å›¾æˆ–å¤šå›¾ï¼‰
                messages = self._build_messages_standard(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    image_data=image_data,
                    video_frames=video_frames,
                    fps=fps
                )
            
            # å°è¯•ä¸»æ¨¡å‹
            try:
                client = self._get_client(model_type=model_type, use_backup=False)
                
                # æ„å»ºè°ƒç”¨å‚æ•°
                call_kwargs = {}
                if temperature is not None:
                    call_kwargs['temperature'] = temperature
                if max_tokens is not None:
                    call_kwargs['max_tokens'] = max_tokens
                
                # JSONæ ¼å¼å“åº” - ä½¿ç”¨bindæ–¹æ³•è®¾ç½®
                if response_format and response_format.get("type") == "json_object":
                    try:
                        # å°è¯•ä½¿ç”¨bindè®¾ç½®response_format
                        client = client.bind(response_format={"type": "json_object"})
                    except Exception as e:
                        self.logger.debug(f"bind response_formatå¤±è´¥: {e}ï¼Œå°†åœ¨promptä¸­è¦æ±‚JSONæ ¼å¼")
                
                # è°ƒç”¨LLM
                response = client.invoke(messages, **call_kwargs)
                response_text = response.content
                
                self.logger.info(f"âœ… LLMè°ƒç”¨æˆåŠŸ: æ¨¡å‹={model_type}, å“åº”é•¿åº¦={len(response_text)}")
                
                # è§£æå“åº”
                if response_format and response_format.get("type") == "json_object":
                    analysis_result = self._parse_json_response(response_text)
                    return LLMServiceResult(
                        success=True,
                        response=response_text,
                        analysis_result=analysis_result
                    )
                else:
                    return LLMServiceResult(
                        success=True,
                        response=response_text
                    )
                    
            except Exception as main_error:
                self.logger.warning(f"âš ï¸ ä¸»æ¨¡å‹è°ƒç”¨å¤±è´¥: {main_error}")
                
                # è‡ªåŠ¨é™çº§åˆ°å¤‡ç”¨æ¨¡å‹
                if settings.LLM_ENABLE_FALLBACK:
                    self.logger.info(f"ğŸ”„ åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹...")
                    
                    try:
                        client = self._get_client(model_type=model_type, use_backup=True)
                        
                        # å¤‡ç”¨æ¨¡å‹è°ƒç”¨
                        call_kwargs = {}
                        if temperature is not None:
                            call_kwargs['temperature'] = temperature
                        if max_tokens is not None:
                            call_kwargs['max_tokens'] = max_tokens
                        
                        # JSONæ ¼å¼å“åº” - ä½¿ç”¨bindæ–¹æ³•è®¾ç½®
                        if response_format and response_format.get("type") == "json_object":
                            try:
                                client = client.bind(response_format={"type": "json_object"})
                            except Exception as e:
                                self.logger.debug(f"bind response_formatå¤±è´¥: {e}ï¼Œå°†åœ¨promptä¸­è¦æ±‚JSONæ ¼å¼")
                        
                        response = client.invoke(messages, **call_kwargs)
                        response_text = response.content
                        
                        self.logger.info(f"âœ… å¤‡ç”¨æ¨¡å‹è°ƒç”¨æˆåŠŸ")
                        
                        # è§£æå“åº”
                        if response_format and response_format.get("type") == "json_object":
                            analysis_result = self._parse_json_response(response_text)
                            return LLMServiceResult(
                                success=True,
                                response=response_text,
                                analysis_result=analysis_result
                            )
                        else:
                            return LLMServiceResult(
                                success=True,
                                response=response_text
                            )
                    
                    except Exception as backup_error:
                        self.logger.error(f"âŒ å¤‡ç”¨æ¨¡å‹ä¹Ÿå¤±è´¥: {backup_error}")
                        raise backup_error
                else:
                    raise main_error
        
        except Exception as e:
            error_msg = f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
            self.logger.error(error_msg, exc_info=True)
            return LLMServiceResult(
                success=False,
                error_message=error_msg
            )
    
    def chat_with_history(
        self,
        prompt: str,
        session_id: str,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> LLMServiceResult:
        """
        å¸¦å†å²è®°å½•çš„å¯¹è¯ï¼ˆä»…æ”¯æŒçº¯æ–‡æœ¬ï¼‰
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            session_id: ä¼šè¯ID
            system_prompt: ç³»ç»Ÿæç¤º
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™call_llm
        """
        try:
            # è·å–å†å²æ¶ˆæ¯
            history = RedisChatMessageHistory(session_id, self.memory_store)
            messages = []
            
            if system_prompt:
                messages.append(SystemMessage(content=system_prompt))
            
            # æ·»åŠ å†å²æ¶ˆæ¯
            messages.extend(history.messages)
            
            # æ·»åŠ å½“å‰è¾“å…¥
            messages.append(HumanMessage(content=prompt))
            
            # è°ƒç”¨LLMï¼ˆçº¯æ–‡æœ¬æ¨¡å‹ï¼‰
            client = self._get_client(model_type="text", use_backup=False)
            response = client.invoke(messages, **kwargs)
            response_text = response.content
            
            # ä¿å­˜åˆ°å†å²
            history.add_message(HumanMessage(content=prompt))
            history.add_message(AIMessage(content=response_text))
            
            return LLMServiceResult(
                success=True,
                response=response_text
            )
        
        except Exception as e:
            error_msg = f"å¯¹è¯å¤±è´¥: {str(e)}"
            self.logger.error(error_msg, exc_info=True)
            return LLMServiceResult(
                success=False,
                error_message=error_msg
            )
    
    def clear_history(self, session_id: str) -> None:
        """æ¸…é™¤ä¼šè¯å†å²"""
        self.memory_store.clear(session_id)


# å…¨å±€å•ä¾‹
llm_service = LLMService()
