#!/usr/bin/env python
# -*- coding: utf-8 -*-

import logging
import asyncio
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, desc

from app.db.session import get_db
from app.models.alert import Alert, AlertCreate, AlertResponse
from app.services.alert_service import connected_clients, DateTimeEncoder
from app.services.rabbitmq_client import rabbitmq_client
from app.core.config import settings

logger = logging.getLogger(__name__)

class MessageRecoveryService:
    """æ¶ˆæ¯æ¢å¤æœåŠ¡ - åˆ©ç”¨MySQLå’ŒRabbitMQæ¢å¤ä¸¢å¤±çš„æ¶ˆæ¯"""
    
    def __init__(self):
        # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°ï¼Œå®ç°å®Œå…¨é…ç½®åŒ–
        self.recovery_window_hours = settings.MESSAGE_RECOVERY_WINDOW_HOURS
        self.batch_size = settings.MESSAGE_RECOVERY_BATCH_SIZE
        self.batch_sleep_seconds = settings.RECOVERY_BATCH_SLEEP_MS / 1000.0
        self.max_messages = settings.DB_RECOVERY_MAX_MESSAGES
        self.max_retry_count = settings.MESSAGE_RECOVERY_MAX_RETRY
        self.timeout_seconds = settings.MESSAGE_RECOVERY_TIMEOUT_SECONDS
        self.send_timeout = settings.RECOVERY_SEND_TIMEOUT_SECONDS
        self.is_recovering = False
        self.last_recovery_time = None
        self.total_recovered = 0
        self.total_failed = 0
        
    async def recover_missing_messages(self, 
                                     start_time: Optional[datetime] = None,
                                     end_time: Optional[datetime] = None,
                                     recovery_mode: str = "auto") -> Dict[str, Any]:
        """
        æ¢å¤ä¸¢å¤±çš„æ¶ˆæ¯
        
        Args:
            start_time: æ¢å¤èµ·å§‹æ—¶é—´ï¼Œé»˜è®¤ä¸º24å°æ—¶å‰
            end_time: æ¢å¤ç»“æŸæ—¶é—´ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¶é—´
            recovery_mode: æ¢å¤æ¨¡å¼ auto/manual/database/deadletter
            
        Returns:
            æ¢å¤ç»“æœç»Ÿè®¡
        """
        if self.is_recovering:
            return {"error": "æ¶ˆæ¯æ¢å¤æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç¨åå†è¯•"}
        
        self.is_recovering = True
        recovery_stats = {
            "start_time": datetime.now().isoformat(),
            "recovery_mode": recovery_mode,
            "database_recovery": {"recovered": 0, "failed": 0, "skipped": 0},
            "deadletter_recovery": {"recovered": 0, "failed": 0, "processed": 0},
            "total_messages_found": 0,
            "total_recovery_attempts": 0,
            "success_rate": 0.0,
            "errors": []
        }
        
        try:
            # è®¾ç½®é»˜è®¤æ—¶é—´èŒƒå›´
            if not end_time:
                end_time = datetime.now()
            if not start_time:
                start_time = end_time - timedelta(hours=self.recovery_window_hours)
            
            logger.info(f"ğŸ”„ å¼€å§‹æ¶ˆæ¯æ¢å¤: {start_time} åˆ° {end_time}, æ¨¡å¼: {recovery_mode}")
            
            # æ ¹æ®æ¢å¤æ¨¡å¼æ‰§è¡Œä¸åŒçš„æ¢å¤ç­–ç•¥
            if recovery_mode in ["auto", "database"]:
                db_stats = await self._recover_from_database(start_time, end_time)
                recovery_stats["database_recovery"] = db_stats
                
            if recovery_mode in ["auto", "deadletter"]:
                dl_stats = await self._recover_from_deadletter_queue()
                recovery_stats["deadletter_recovery"] = dl_stats
            
            if recovery_mode == "manual":
                # æ‰‹åŠ¨æ¢å¤æ¨¡å¼ï¼Œéœ€è¦ç”¨æˆ·æŒ‡å®šå…·ä½“çš„æ¶ˆæ¯IDæˆ–æ¡ä»¶
                manual_stats = await self._manual_recovery(start_time, end_time)
                recovery_stats["manual_recovery"] = manual_stats
            
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            total_recovered = (recovery_stats["database_recovery"]["recovered"] + 
                             recovery_stats["deadletter_recovery"]["recovered"])
            total_attempts = (recovery_stats["database_recovery"]["recovered"] + 
                            recovery_stats["database_recovery"]["failed"] +
                            recovery_stats["deadletter_recovery"]["processed"])
            
            recovery_stats["total_recovery_attempts"] = total_attempts
            recovery_stats["success_rate"] = (total_recovered / total_attempts * 100) if total_attempts > 0 else 0
            recovery_stats["end_time"] = datetime.now().isoformat()
            
            # æ›´æ–°å®ä¾‹ç»Ÿè®¡ä¿¡æ¯
            self.last_recovery_time = datetime.now()
            self.total_recovered += total_recovered
            self.total_failed += (recovery_stats["database_recovery"]["failed"] + 
                                recovery_stats["deadletter_recovery"]["failed"])
            
            logger.info(f"âœ… æ¶ˆæ¯æ¢å¤å®Œæˆ: æ¢å¤ {total_recovered} æ¡æ¶ˆæ¯ï¼ŒæˆåŠŸç‡ {recovery_stats['success_rate']:.1f}%")
            
        except Exception as e:
            error_msg = f"æ¶ˆæ¯æ¢å¤è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}"
            logger.error(error_msg, exc_info=True)
            recovery_stats["errors"].append(error_msg)
            recovery_stats["end_time"] = datetime.now().isoformat()
        finally:
            self.is_recovering = False
            
        return recovery_stats
    
    async def _recover_from_database(self, start_time: datetime, end_time: datetime) -> Dict[str, int]:
        """ä»MySQLæ•°æ®åº“æ¢å¤æ¶ˆæ¯"""
        stats = {"recovered": 0, "failed": 0, "skipped": 0}
        
        try:
            logger.info(f"ğŸ“‹ ä»æ•°æ®åº“æ¢å¤æ¶ˆæ¯: {start_time} åˆ° {end_time}")
            
            # ä½¿ç”¨æ­£ç¡®çš„æ•°æ®åº“ä¼šè¯è·å–æ–¹å¼
            db_generator = get_db()
            db = next(db_generator)
            
            try:
                # æŸ¥è¯¢æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æŠ¥è­¦æ¶ˆæ¯
                alerts = (db.query(Alert)
                         .filter(and_(
                             Alert.alert_time >= start_time,
                             Alert.alert_time <= end_time
                         ))
                         .order_by(Alert.alert_time.asc())
                         .limit(self.max_messages)
                         .all())
                
                logger.info(f"ğŸ“Š æ•°æ®åº“ä¸­æ‰¾åˆ° {len(alerts)} æ¡æŠ¥è­¦è®°å½•")
                
                # æ‰¹é‡å¤„ç†æ¢å¤
                for i in range(0, len(alerts), self.batch_size):
                    batch = alerts[i:i + self.batch_size]
                    batch_stats = await self._process_alert_batch(batch, "database_recovery")
                    
                    stats["recovered"] += batch_stats["recovered"]
                    stats["failed"] += batch_stats["failed"]
                    stats["skipped"] += batch_stats["skipped"]
                    
                    # ä½¿ç”¨é…ç½®çš„å»¶è¿Ÿæ—¶é—´é¿å…ç³»ç»Ÿè¿‡è½½
                    await asyncio.sleep(self.batch_sleep_seconds)
                    
                    logger.debug(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i//self.batch_size + 1}: "
                               f"æ¢å¤={batch_stats['recovered']}, "
                               f"å¤±è´¥={batch_stats['failed']}, "
                               f"è·³è¿‡={batch_stats['skipped']}")
                               
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“æ¢å¤å¤±è´¥: {str(e)}")
            stats["failed"] = stats.get("failed", 0) + 1
            
        return stats
    
    async def _recover_from_deadletter_queue(self) -> Dict[str, int]:
        """ä»RabbitMQæ­»ä¿¡é˜Ÿåˆ—æ¢å¤æ¶ˆæ¯"""
        stats = {"recovered": 0, "failed": 0, "processed": 0}
        
        try:
            logger.info("ğŸ’€ ä»æ­»ä¿¡é˜Ÿåˆ—æ¢å¤æ¶ˆæ¯")
            
            # è·å–æ­»ä¿¡é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰æ¶ˆæ¯
            dead_messages = rabbitmq_client.get_dead_letter_messages(max_count=1000)
            stats["processed"] = len(dead_messages)
            
            if not dead_messages:
                logger.info("ğŸ“­ æ­»ä¿¡é˜Ÿåˆ—ä¸ºç©º")
                return stats
            
            logger.info(f"ğŸ“‹ æ­»ä¿¡é˜Ÿåˆ—ä¸­æ‰¾åˆ° {len(dead_messages)} æ¡æ¶ˆæ¯")
            
            # å¤„ç†æ¯æ¡æ­»ä¿¡æ¶ˆæ¯
            for dead_info in dead_messages:
                try:
                    message_data = dead_info['message_data']
                    delivery_tag = dead_info['delivery_tag']
                    
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ¢å¤è¿™æ¡æ¶ˆæ¯
                    if self._should_recover_message(dead_info):
                        # é‡æ–°å¤„ç†æ­»ä¿¡æ¶ˆæ¯
                        success = await self._reprocess_dead_message(dead_info)
                        
                        if success:
                            stats["recovered"] += 1
                            # ç¡®è®¤æ­»ä¿¡æ¶ˆæ¯å·²å¤„ç†
                            rabbitmq_client.channel.basic_ack(delivery_tag=delivery_tag)
                            logger.debug(f"âœ… æ­»ä¿¡æ¶ˆæ¯æ¢å¤æˆåŠŸ: {message_data.get('alert_type', 'unknown')}")
                        else:
                            stats["failed"] += 1
                            # æ‹’ç»æ¶ˆæ¯ä½†ä¸é‡æ–°å…¥é˜Ÿ
                            rabbitmq_client.channel.basic_nack(delivery_tag=delivery_tag, requeue=False)
                            logger.warning(f"âŒ æ­»ä¿¡æ¶ˆæ¯æ¢å¤å¤±è´¥: {message_data.get('alert_type', 'unknown')}")
                    else:
                        # è·³è¿‡ä¸éœ€è¦æ¢å¤çš„æ¶ˆæ¯
                        rabbitmq_client.channel.basic_ack(delivery_tag=delivery_tag)
                        logger.debug(f"â­ï¸ è·³è¿‡æ­»ä¿¡æ¶ˆæ¯: {message_data.get('alert_type', 'unknown')}")
                        
                except Exception as e:
                    stats["failed"] += 1
                    logger.error(f"âŒ å¤„ç†æ­»ä¿¡æ¶ˆæ¯å¼‚å¸¸: {str(e)}")
                    
        except Exception as e:
            logger.error(f"âŒ æ­»ä¿¡é˜Ÿåˆ—æ¢å¤å¤±è´¥: {str(e)}")
            
        return stats
    
    async def _manual_recovery(self, start_time: datetime, end_time: datetime) -> Dict[str, int]:
        """æ‰‹åŠ¨æ¢å¤æ¨¡å¼ - ç”¨æˆ·æŒ‡å®šç‰¹å®šæ¡ä»¶æ¢å¤"""
        stats = {"recovered": 0, "failed": 0, "skipped": 0}
        
        try:
            logger.info("ğŸ”§ æ‰§è¡Œæ‰‹åŠ¨æ¢å¤æ¨¡å¼")
            
            # è¿™é‡Œå¯ä»¥æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„æ¡ä»¶è¿›è¡Œæ¢å¤
            # ä¾‹å¦‚ï¼šç‰¹å®šçš„alert_typeã€camera_idã€alert_levelç­‰
            
            # ç¤ºä¾‹ï¼šæ¢å¤é«˜çº§åˆ«æŠ¥è­¦
            db_generator = get_db()
            db = next(db_generator)
            
            try:
                high_priority_alerts = (db.query(Alert)
                                      .filter(and_(
                                          Alert.alert_time >= start_time,
                                          Alert.alert_time <= end_time,
                                          Alert.alert_level >= settings.DEAD_LETTER_HIGH_PRIORITY_LEVEL
                                      ))
                                      .order_by(Alert.alert_time.asc())
                                      .limit(self.max_messages)
                                      .all())
                
                logger.info(f"ğŸ”¥ æ‰¾åˆ° {len(high_priority_alerts)} æ¡é«˜çº§åˆ«æŠ¥è­¦éœ€è¦æ¢å¤")
                
                batch_stats = await self._process_alert_batch(high_priority_alerts, "manual_recovery")
                stats.update(batch_stats)
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"âŒ æ‰‹åŠ¨æ¢å¤å¤±è´¥: {str(e)}")
            stats["failed"] += 1
            
        return stats
    
    async def _process_alert_batch(self, alerts: List[Alert], recovery_source: str) -> Dict[str, int]:
        """æ‰¹é‡å¤„ç†æŠ¥è­¦æ¶ˆæ¯"""
        stats = {"recovered": 0, "failed": 0, "skipped": 0}
        
        for alert in alerts:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰SSEå®¢æˆ·ç«¯è¿æ¥
                if not connected_clients:
                    stats["skipped"] += 1
                    continue
                
                # è½¬æ¢ä¸ºAlertResponseæ ¼å¼
                alert_dict = AlertResponse.from_orm(alert).dict()
                
                # æ·»åŠ æ¢å¤æ ‡è¯†
                alert_dict['is_recovery'] = True
                alert_dict['recovery_source'] = recovery_source
                alert_dict['recovery_time'] = datetime.now().isoformat()
                alert_dict['original_timestamp'] = alert.alert_time.isoformat()
                
                # æ„é€ SSEæ¶ˆæ¯
                message = json.dumps(alert_dict, cls=DateTimeEncoder)
                sse_message = f"data: {message}\n\n"
                
                # å‘é€ç»™æ‰€æœ‰å®¢æˆ·ç«¯
                success = await self._broadcast_recovery_message(sse_message)
                
                if success:
                    stats["recovered"] += 1
                    logger.debug(f"ğŸ“¤ æ¢å¤æ¶ˆæ¯å·²å¹¿æ’­: ID={alert.id}, ç±»å‹={alert.alert_type}")
                else:
                    stats["failed"] += 1
                    logger.warning(f"âŒ æ¢å¤æ¶ˆæ¯å¹¿æ’­å¤±è´¥: ID={alert.id}")
                    
            except Exception as e:
                stats["failed"] += 1
                logger.error(f"âŒ å¤„ç†æŠ¥è­¦æ¶ˆæ¯å¤±è´¥: ID={alert.id}, é”™è¯¯: {str(e)}")
        
        return stats
    
    async def _broadcast_recovery_message(self, sse_message: str) -> bool:
        """å¹¿æ’­æ¢å¤æ¶ˆæ¯åˆ°æ‰€æœ‰SSEå®¢æˆ·ç«¯"""
        if not connected_clients:
            return False
        
        try:
            tasks = []
            for client_queue in connected_clients.copy():
                task = asyncio.create_task(self._safe_send_to_client(client_queue, sse_message))
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            success_count = sum(1 for result in results if result is True)
            
            return success_count > 0
        except Exception as e:
            logger.error(f"âŒ å¹¿æ’­æ¢å¤æ¶ˆæ¯å¤±è´¥: {str(e)}")
            return False
    
    async def _safe_send_to_client(self, client_queue: asyncio.Queue, message: str) -> bool:
        """å®‰å…¨å‘é€æ¶ˆæ¯åˆ°å®¢æˆ·ç«¯"""
        try:
            await asyncio.wait_for(client_queue.put(message), timeout=self.send_timeout)
            return True
        except (asyncio.TimeoutError, Exception):
            return False
    
    def _should_recover_message(self, dead_info: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ­»ä¿¡æ¶ˆæ¯æ˜¯å¦åº”è¯¥æ¢å¤"""
        try:
            message_data = dead_info.get('message_data', {})
            dead_reason = dead_info.get('dead_reason', '')
            retry_count = dead_info.get('retry_count', 0)
            death_count = dead_info.get('death_count', 0)
            
            # 1. è·³è¿‡é‡è¯•æ¬¡æ•°è¿‡å¤šçš„æ¶ˆæ¯
            if retry_count > settings.DEADLETTER_RECOVERY_MAX_RETRY_COUNT:
                return False
            
            # 2. è·³è¿‡æ­»ä¿¡æ¬¡æ•°è¿‡å¤šçš„æ¶ˆæ¯
            if death_count > settings.DEADLETTER_RECOVERY_MAX_DEATH_COUNT:
                return False
            
            # 3. æ ¹æ®æ­»ä¿¡åŸå› åˆ¤æ–­
            if dead_reason in ['rejected', 'expired']:
                # å¯¹äºè¢«æ‹’ç»æˆ–è¿‡æœŸçš„æ¶ˆæ¯ï¼Œæ ¹æ®é‡è¦æ€§åˆ¤æ–­
                alert_level = message_data.get('alert_level', 1)
                return alert_level >= settings.RECOVERY_MIN_ALERT_LEVEL
            
            # 4. å…¶ä»–æƒ…å†µé»˜è®¤æ¢å¤
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ¤æ–­æ¶ˆæ¯æ¢å¤æ¡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    async def _reprocess_dead_message(self, dead_info: Dict[str, Any]) -> bool:
        """é‡æ–°å¤„ç†æ­»ä¿¡æ¶ˆæ¯"""
        try:
            message_data = dead_info['message_data']
            
            # é‡æ–°åˆ›å»ºAlertå¯¹è±¡å¹¶ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¦‚æœéœ€è¦ï¼‰
            alert_create = AlertCreate(**message_data)
            
            db_generator = get_db()
            db = next(db_generator)
            
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„æŠ¥è­¦è®°å½•
                existing_alert = (db.query(Alert)
                                .filter(and_(
                                    Alert.alert_time == alert_create.alert_time,
                                    Alert.camera_id == alert_create.camera_id,
                                    Alert.alert_type == alert_create.alert_type
                                ))
                                .first())
                
                if existing_alert:
                    # å¦‚æœå·²å­˜åœ¨ï¼Œç›´æ¥å¹¿æ’­ç°æœ‰è®°å½•
                    alert_dict = AlertResponse.from_orm(existing_alert).dict()
                else:
                    # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°è®°å½•
                    from app.services.alert_service import alert_service
                    new_alert = alert_service.create_alert(db, alert_create)
                    alert_dict = AlertResponse.from_orm(new_alert).dict()
                
                # æ·»åŠ æ¢å¤æ ‡è¯†
                alert_dict['is_recovery'] = True
                alert_dict['recovery_source'] = 'deadletter_queue'
                alert_dict['recovery_time'] = datetime.now().isoformat()
                
                # æ„é€ SSEæ¶ˆæ¯å¹¶å¹¿æ’­
                message = json.dumps(alert_dict, cls=DateTimeEncoder)
                sse_message = f"data: {message}\n\n"
                
                return await self._broadcast_recovery_message(sse_message)
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"âŒ é‡æ–°å¤„ç†æ­»ä¿¡æ¶ˆæ¯å¤±è´¥: {str(e)}")
            return False
    
    async def check_message_consistency(self, 
                                      start_time: Optional[datetime] = None,
                                      end_time: Optional[datetime] = None) -> Dict[str, Any]:
        """æ£€æŸ¥æ¶ˆæ¯ä¸€è‡´æ€§ï¼Œå‘ç°å¯èƒ½ä¸¢å¤±çš„æ¶ˆæ¯"""
        if not end_time:
            end_time = datetime.now()
        if not start_time:
            start_time = end_time - timedelta(hours=1)
        
        consistency_report = {
            "check_period": {
                "start": start_time.isoformat(),
                "end": end_time.isoformat()
            },
            "database_messages": 0,
            "deadletter_messages": 0,
            "potential_losses": [],
            "recommendations": []
        }
        
        try:
            # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ¶ˆæ¯æ•°é‡
            db_generator = get_db()
            db = next(db_generator)
            
            try:
                db_count = (db.query(Alert)
                           .filter(and_(
                               Alert.alert_time >= start_time,
                               Alert.alert_time <= end_time
                           ))
                           .count())
                consistency_report["database_messages"] = db_count
            finally:
                db.close()
            
            # æ£€æŸ¥æ­»ä¿¡é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯
            dead_messages = rabbitmq_client.get_dead_letter_messages(max_count=1000)
            consistency_report["deadletter_messages"] = len(dead_messages)
            
            # åˆ†ææ½œåœ¨çš„æ¶ˆæ¯ä¸¢å¤±
            if consistency_report["deadletter_messages"] > 0:
                consistency_report["potential_losses"].append(
                    f"æ­»ä¿¡é˜Ÿåˆ—ä¸­æœ‰ {len(dead_messages)} æ¡æ¶ˆæ¯å¯èƒ½æœªæ­£ç¡®å¤„ç†"
                )
            
            # ç”Ÿæˆå»ºè®®
            if consistency_report["deadletter_messages"] > 10:
                consistency_report["recommendations"].append("å»ºè®®æ‰§è¡Œæ­»ä¿¡é˜Ÿåˆ—æ¢å¤")
            
            if consistency_report["database_messages"] == 0 and consistency_report["deadletter_messages"] > 0:
                consistency_report["recommendations"].append("å¯èƒ½å­˜åœ¨æ•°æ®åº“è¿æ¥é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®åº“çŠ¶æ€")
            
        except Exception as e:
            logger.error(f"âŒ æ¶ˆæ¯ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {str(e)}")
            consistency_report["error"] = str(e)
        
        return consistency_report
    
    def get_recovery_status(self) -> Dict[str, Any]:
        """è·å–æ¢å¤æœåŠ¡çŠ¶æ€"""
        return {
            "is_recovering": self.is_recovering,
            "last_recovery_time": self.last_recovery_time.isoformat() if self.last_recovery_time else None,
            "total_recovered": self.total_recovered,
            "total_failed": self.total_failed,
            "recovery_window_hours": self.recovery_window_hours,
            "batch_size": self.batch_size,
            "max_messages": self.max_messages,
            "batch_sleep_seconds": self.batch_sleep_seconds,
            "send_timeout": self.send_timeout,
            "max_retry_count": self.max_retry_count,
            "timeout_seconds": self.timeout_seconds,
            "connected_clients": len(connected_clients),
            "status": "running" if self.is_recovering else "idle",
            "config_source": "settings_file",
            "performance_stats": {
                "success_rate": (self.total_recovered / (self.total_recovered + self.total_failed) * 100) 
                               if (self.total_recovered + self.total_failed) > 0 else 0,
                "avg_batch_size": self.batch_size,
                "max_concurrent_messages": self.max_messages
            },
            "deadletter_queue_stats": rabbitmq_client.get_dead_letter_queue_stats() if rabbitmq_client else {}
        }

# åˆ›å»ºå…¨å±€æ¶ˆæ¯æ¢å¤æœåŠ¡å®ä¾‹
message_recovery_service = MessageRecoveryService()

# å¤–éƒ¨æ¥å£å‡½æ•°
async def recover_missing_messages(start_time: Optional[datetime] = None,
                                 end_time: Optional[datetime] = None,
                                 recovery_mode: str = "auto") -> Dict[str, Any]:
    """æ¢å¤ä¸¢å¤±æ¶ˆæ¯çš„å¤–éƒ¨æ¥å£"""
    return await message_recovery_service.recover_missing_messages(start_time, end_time, recovery_mode)

async def check_message_consistency(start_time: Optional[datetime] = None,
                                  end_time: Optional[datetime] = None) -> Dict[str, Any]:
    """æ£€æŸ¥æ¶ˆæ¯ä¸€è‡´æ€§çš„å¤–éƒ¨æ¥å£"""
    return await message_recovery_service.check_message_consistency(start_time, end_time)

def get_recovery_status() -> Dict[str, Any]:
    """è·å–æ¢å¤çŠ¶æ€çš„å¤–éƒ¨æ¥å£"""
    return message_recovery_service.get_recovery_status() 